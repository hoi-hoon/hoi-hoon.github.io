---
title:  "[공부 노트] #6"
excerpt: "1월 4주차 공부 노트"

categories:
  - Study
tags:
  - Study Note
use_math: true
last_modified_at: 2021-01-30T12:00:00
---


## Q1. 언어 모델
언어모델이란 단어 시퀀스에 대한 확률분포(probability distribution)를 말한다. 단어의 발생 빈도를 세서 확률을 구하는 통계적 언어 모델(Statistical Language Model, SLM)은 학습 코퍼스에 매우 의존적이다. 학습하는 코퍼스에 해당 단어가 없다면 단어의 등장 확률은 0이 되기도 한다. 따라서 n-gram, 스무딩, 평탄화와 같은 일반화 작업이 등장하였으나, 근본적인 해결책은 되지 못 하였고 인공 신경망 언어 모델로 경향이 넘어가게 되었다.

## Q2. TF-IDF
- TF(Term Frequency) : 특정 문서에서 특정 단어의 등장 횟수
- DF(Document Frequency) : 특정 단어가 등장한 문서의 수
- IDF(Inverse Document Frequency) : 총 문서 수를 DF로 나눈 뒤 로그를 취한 값. 그 값이 클수록 특이한(희귀한) 단어라는 것

TF-IDF는 TF와 IDF를 곱해 두 지표를 동시에 고려하는 가중치 산출 방법이다. 
> $TF-IDF(w) = tf(w)*log(\frac{N}{df(w)})$

## Q3. 코사인 유사도(Cosine Similarity)
> $similarity = cos(Θ) = \frac{A \cdot B}{\Vert A \Vert \Vert B \Vert}$

코사인 유사도에 대해서 잘못 이해했나해서 굉장히 많이 찾아보았다. 내가 헷갈린 부분은
```
cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)
```
라는 코드 때문이었다. skit-learn 패키지의 함수로, 찾아보면 dot product를 수행해주는 함수라고 하는데, 여기서 궁금증이 시작된 것이다. 코사인 유사도를 구하는 식을 굳이 쓰자면
```python
def cos_sim(A, B):
       return dot(A, B)/(norm(A)*norm(B))
```
인데, linear_kernel 함수는 (norm(A)*norm(B)) 로 나눠주는 부분이 없는데 도대체 왜 같은 걸까?

```python
cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)
cosine_sim_t = tfidf_matrix * tfidf_matrix.T
print(cosine_sim)
print(cosine_sim_t.toarray())
```
```
[[1.         0.01575748 0.         ... 0.         0.         0.        ]
 [0.01575748 1.         0.04907345 ... 0.         0.         0.        ]
 [0.         0.04907345 1.         ... 0.         0.         0.        ]
 ...
 [0.         0.         0.         ... 1.         0.         0.08375766]
 [0.         0.         0.         ... 0.         1.         0.        ]
 [0.         0.         0.         ... 0.08375766 0.         1.        ]]
[[1.         0.01575748 0.         ... 0.         0.         0.        ]
 [0.01575748 1.         0.04907345 ... 0.         0.         0.        ]
 [0.         0.04907345 1.         ... 0.         0.         0.        ]
 ...
 [0.         0.         0.         ... 1.         0.         0.08375766]
 [0.         0.         0.         ... 0.         1.         0.        ]
 [0.         0.         0.         ... 0.08375766 0.         1.        ]]
```
한눈에 보기에도 linear_kernel 함수와 내적은 동일한 결과를 갖는다. Cosine similarity 공식 문서에는 L2 Normalize 된 데이터에 대해서는 두 데이터를 곱하는 것만(Linear_kernel)으로도 코사인 유사도를 구할 수 있다고 나와있다. 
> **L2 Normalize 된 데이터**

그러니까 sklearn에서 제공하는 TfidfVectorizer는 L2 norm 까지 거친 tf-idf vector를 반환해주는 것이다.


## MathJax
NLP관련 공부를 하다보니 수식을 적어야 할 일이 많아져서 찾아보게 되었다. MathJax를 이용하면 Jekyll Github 블로그에서 수학 수식을 표현할 수 있고, 그 적용 방법 또한 매우 간단했다. 적용 방법은 하단에 링크를 첨부하였다.

참고문헌
> https://wikidocs.net/21698  
https://mkkim85.github.io/blog-apply-mathjax-to-jekyll-and-github-pages/  
https://ratsgo.github.io/